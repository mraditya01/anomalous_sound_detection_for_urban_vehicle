{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0579bcc9-946a-45eb-ac59-d5339828ba81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/usr/local/lib/python3.9/dist-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/usr/local/lib/python3.9/dist-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl5mutexC1Ev']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/usr/local/lib/python3.9/dist-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/usr/local/lib/python3.9/dist-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/usr/local/lib/python3.9/dist-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZNK10tensorflow4data11DatasetBase8FinalizeEPNS_15OpKernelContextESt8functionIFN3tsl8StatusOrISt10unique_ptrIS1_NS5_4core15RefCountDeleterEEEEvEE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# import default libraries\n",
    "########################################################################\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "########################################################################\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# import additional libraries\n",
    "########################################################################\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "# from import\n",
    "\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    from sklearn.externals import joblib\n",
    "except:\n",
    "    import joblib\n",
    "# original lib\n",
    "import keras_model_facenet\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "import keras.backend as K\n",
    "import common as com\n",
    "from __future__ import division \n",
    "import numpy as np\n",
    "import scipy.signal as sps\n",
    "import scipy.io.wavfile as wf\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd2650db-d163-4dd7-bee9-002ed511add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# load parameter.yaml\n",
    "########################################################################\n",
    "param = com.yaml_load()\n",
    "########################################################################\n",
    "saved_weight = os.path.join(param[\"P_MODELSAVE\"], 'dataweights.{epoch:02d}-{val_dense_1_binary_accuracy:.2f}.hdf5')\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor = 'loss', patience = 1000)\n",
    "# model_unet = keras_model.get_model(input_shape=(1,param[\"feature\"][\"n_mels\"],param[\"feature\"][\"n_frames\"]), lr = param[\"fit\"][\"lr\"])\n",
    "\n",
    "########################################################################\n",
    "# get data from the list for file paths\n",
    "########################################################################\n",
    "def get_label(file_path):\n",
    "  # Convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    parts = tf.strings.split(parts[-1], sep='_')\n",
    "    num = tf.strings.to_number(parts[1], out_type=tf.int64)\n",
    "    return num\n",
    "\n",
    "def get_name(file_path):\n",
    "  # Convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    parts = tf.strings.split(parts[-1])\n",
    "    return parts\n",
    "\n",
    "def file_high(file_name,\n",
    "                n_mels=128,\n",
    "                n_fft=1024,\n",
    "                hop_length=431,\n",
    "                power=2.0):\n",
    "    \"\"\"\n",
    "    convert file_name to a vector array.\n",
    "\n",
    "    file_name : str\n",
    "        target .wav file\n",
    "\n",
    "    return : numpy.array( numpy.array( float ) )\n",
    "        vector array\n",
    "        * dataset.shape = (dataset_size, feature_vector_length)\n",
    "    \"\"\"\n",
    "    # Dictionary\n",
    "    table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "    keys=tf.constant([2, 3, 5, 7, 1, 4, 6, 8]),\n",
    "    values=tf.constant([0, 1, 2, 3, 0, 1, 2, 3]),\n",
    "    ),\n",
    "    default_value=tf.constant(-1),\n",
    "    name=\"class_weight\"\n",
    "    )\n",
    "    label1 = get_label(file_name)\n",
    "    # generate melspectrogram using tf.io\n",
    "    file_contents = tf.io.read_file(file_name)\n",
    "    data, sr = tf.audio.decode_wav(file_contents, desired_channels=1)\n",
    "    data = tf.squeeze(data, axis=[-1])\n",
    "    n = tf.split(data, 5)\n",
    "    # label1 = tf.one_hot(int(label1), 8)\n",
    "    label1 = tf.one_hot(table.lookup(int(label1)), 4)\n",
    "    label2 = tf.one_hot(0, 1)\n",
    "    return n[0],n[1],n[2],n[3],n[4], label2, tf.concat([label1, label2], 0)\n",
    "\n",
    "\n",
    "def file_low(file_name):\n",
    "    \"\"\"\n",
    "    convert file_name to a vector array.\n",
    "\n",
    "    file_name : str\n",
    "        target .wav file\n",
    "\n",
    "    return : numpy.array( numpy.array( float ) )\n",
    "        vector array\n",
    "        * dataset.shape = (dataset_size, feature_vector_length)\n",
    "    \"\"\"\n",
    "    table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "    keys=tf.constant([2, 3, 5, 7, 1, 4, 6, 8]),\n",
    "    values=tf.constant([0, 1, 2, 3, 0, 1, 2, 3]),\n",
    "    ),\n",
    "    default_value=tf.constant(-1),\n",
    "    name=\"class_weight\"\n",
    "    )\n",
    "    label1 = get_label(file_name)\n",
    "    # generate melspectrogram using tf.io\n",
    "    file_contents = tf.io.read_file(file_name)\n",
    "    data, sr = tf.audio.decode_wav(file_contents, desired_channels=1)\n",
    "    data = tf.squeeze(data, axis=[-1])\n",
    "    n = tf.split(data, 5)\n",
    "    # label1 = tf.one_hot(int(label1), 8)\n",
    "    label1 = tf.one_hot(table.lookup(int(label1)), 4)\n",
    "    label2 = tf.one_hot(1, 1)\n",
    "    return n[0],n[1],n[2],n[3],n[4], label2, tf.concat([label1, label2], 0)\n",
    "\n",
    "\n",
    "def file_ano(file_name):\n",
    "    \"\"\"\n",
    "    convert file_name to a vector array.\n",
    "\n",
    "    file_name : str\n",
    "        target .wav file\n",
    "\n",
    "    return : numpy.array( numpy.array( float ) )\n",
    "        vector array\n",
    "        * dataset.shape = (dataset_size, feature_vector_length)\n",
    "    \"\"\"\n",
    "    table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "    keys=tf.constant([2, 3, 5, 7, 1, 4, 6, 8]),\n",
    "    values=tf.constant([0, 1, 2, 3, 0, 1, 2, 3]),\n",
    "    ),\n",
    "    default_value=tf.constant(-1),\n",
    "    name=\"class_weight\"\n",
    "    )\n",
    "    # generate melspectrogram using tf.io\n",
    "    file_contents = tf.io.read_file(file_name)\n",
    "    # label1 = tf.one_hot(int(label1), 8)\n",
    "    label7 = tf.one_hot(3, 4)\n",
    "    label5 = tf.one_hot(2, 4)\n",
    "    label3 = tf.one_hot(1, 4)\n",
    "    label2 = tf.one_hot(0, 4)\n",
    "    \n",
    "    \n",
    "    label = tf.one_hot(1, 1)\n",
    "    data, sr = tf.audio.decode_wav(file_contents)\n",
    "    data = tf.transpose(data)\n",
    "    return data[0], data[1], data[2], data[3], label7, label5, label2, label3, label \n",
    "    # return data, label2, tf.concat([label1, label2], 0)\n",
    "\n",
    "\n",
    "def to_mel(y, machine, section,\n",
    "        anomaly = False,\n",
    "        n_mels=128,\n",
    "        n_fft=2823,\n",
    "        hop_length=347,\n",
    "        power=2.0,\n",
    "        sr=22050):\n",
    "    \n",
    "    spectrogram = tfio.audio.spectrogram(y, nfft=n_fft, window=n_fft, stride=hop_length)\n",
    "    mel_spectrogram = tfio.audio.melscale(spectrogram, sr, mels=n_mels, fmin=0, fmax=11025, name=None)\n",
    "    dbscale_mel_spectrogram = tfio.audio.dbscale(mel_spectrogram, top_db=120)\n",
    "    if anomaly == True:\n",
    "        dbscale_mel_spectrogram = tf.transpose(dbscale_mel_spectrogram)\n",
    "    dbscale_mel_spectrogram = tf.expand_dims(dbscale_mel_spectrogram, axis=2)\n",
    "    return dbscale_mel_spectrogram, {'affine': machine, 'linear': section}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76f6633e-1cea-4d5b-9b49-faed6f2ec4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================\n",
      "============== DATASET_GENERATOR ==============\n",
      "4251 4252\n",
      "3400 3401 3261\n"
     ]
    }
   ],
   "source": [
    "target_dir1 = \"dev_data/high_noise\"\n",
    "target_dir2 = \"dev_data/low_noise\"\n",
    "target_dir3 = \"dev_data/mid_noise\"\n",
    "target_dir4 = \"dev_data/anomalous\"\n",
    "target_dir5 = \"dev_data_doa/combined_ground_truth/train\"\n",
    "# loop of the base directory\n",
    "print(\"\\n===========================\")\n",
    "\n",
    "# generate dataset\n",
    "print(\"============== DATASET_GENERATOR ==============\")\n",
    "\n",
    "# get file list for all sections\n",
    "# all values of y_true are zero in training\n",
    "# Normal\n",
    "pos1 = tf.data.Dataset.list_files(f\"{target_dir1}/train/section_02*.wav\")\n",
    "pos2 = tf.data.Dataset.list_files(f\"{target_dir1}/train/section_03*.wav\")\n",
    "pos3 = tf.data.Dataset.list_files(f\"{target_dir3}/train/section_05*.wav\")\n",
    "pos4 = tf.data.Dataset.list_files(f\"{target_dir3}/train/section_07*.wav\")\n",
    "\n",
    "# Other Noise\n",
    "pos5 = tf.data.Dataset.list_files(f\"{target_dir2}/train/*.wav\")\n",
    "pos6 = tf.data.Dataset.list_files(f\"{target_dir1}/train/section_04*.wav\")\n",
    "\n",
    "# Anomaly\n",
    "real_anomalous = tf.data.Dataset.list_files(f\"{target_dir5}/*.wav\")\n",
    "\n",
    "# pos1 = pos1.concatenate(pos_l).concatenate(pos_ll).concatenate(pos_lll).shuffle(3000)\n",
    "# pos2 = pos2.concatenate(pos_l2).shuffle(5000)\n",
    "normal = pos1.concatenate(pos2).concatenate(pos3).concatenate(pos4).shuffle(5000)\n",
    "anomaly = pos5.concatenate(pos6).shuffle(5000)\n",
    "anomaly2 = real_anomalous.shuffle(5000)\n",
    "print(normal.cardinality().numpy() ,anomaly.cardinality().numpy())\n",
    "train_size_anomaly = int(0.8 * anomaly.cardinality().numpy())\n",
    "train_size_normal = int(0.8 * normal.cardinality().numpy())\n",
    "train_size_real_anomaly = int(0.8 * anomaly2.cardinality().numpy())\n",
    "\n",
    "print(train_size_normal, train_size_anomaly, train_size_real_anomaly) \n",
    "\n",
    "train_dataset_normal = normal.take(train_size_normal)\n",
    "train_dataset_anomaly = anomaly.take(train_size_anomaly)\n",
    "train_dataset_real_anomaly = anomaly2.take(train_size_real_anomaly)\n",
    "\n",
    "val_dataset_normal = normal.skip(train_size_normal)\n",
    "val_dataset_anomaly = anomaly.skip(train_size_anomaly)\n",
    "val_dataset_real_anomaly = anomaly2.skip(train_size_real_anomaly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a61d7e5c-b214-4e9c-bf1c-18af2e561ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############## TRAIN\n",
    "\n",
    "# High Noise\n",
    "dataset_high = train_dataset_normal.map(file_high).map(lambda a, b, c, d, e, x, y: (a, x, y, False))\n",
    "dataset_high2 = train_dataset_normal.map(file_high).map(lambda a, b, c, d, e, x, y: (b, x, y, False))\n",
    "dataset_high3 = train_dataset_normal.map(file_high).map(lambda a, b, c, d, e, x, y: (c, x, y, False))\n",
    "dataset_high4 = train_dataset_normal.map(file_high).map(lambda a, b, c, d, e, x, y: (d, x, y, False))\n",
    "dataset_high5 = train_dataset_normal.map(file_high).map(lambda a, b, c, d, e, x, y: (e, x, y, False))\n",
    "\n",
    "dataset_train_high = dataset_high.concatenate(dataset_high2).concatenate(dataset_high3).concatenate(dataset_high4).concatenate(dataset_high5)\n",
    "\n",
    "# Low Noise\n",
    "dataset_mid = train_dataset_anomaly.map(file_low).map(lambda a, b, c, d, e, x, y: (a, x, y, False))\n",
    "dataset_mid2 = train_dataset_anomaly.map(file_low).map(lambda a, b, c, d, e, x, y: (b, x, y, False))\n",
    "dataset_mid3 = train_dataset_anomaly.map(file_low).map(lambda a, b, c, d, e, x, y: (c, x, y, False))\n",
    "dataset_mid4 = train_dataset_anomaly.map(file_low).map(lambda a, b, c, d, e, x, y: (d, x, y, False))\n",
    "dataset_mid5 = train_dataset_anomaly.map(file_low).map(lambda a, b, c, d, e, x, y: (e, x, y, False))\n",
    "\n",
    "dataset_train_mid = dataset_mid.concatenate(dataset_mid2).concatenate(dataset_mid3).concatenate(dataset_mid4).concatenate(dataset_mid5)\n",
    "\n",
    "# Anomaly\n",
    "dataset_ano1 = train_dataset_real_anomaly.map(file_ano).map(lambda a, b, c, d, x, y, z, i, j: (a, j, tf.concat([x, j], 0), True))\n",
    "dataset_ano2 = train_dataset_real_anomaly.map(file_ano).map(lambda a, b, c, d, x, y, z, i, j: (b, j, tf.concat([y, j], 0), True))\n",
    "dataset_ano3 = train_dataset_real_anomaly.map(file_ano).map(lambda a, b, c, d, x, y, z, i, j: (c, j, tf.concat([z, j], 0), True))\n",
    "dataset_ano4 = train_dataset_real_anomaly.map(file_ano).map(lambda a, b, c, d, x, y, z, i, j: (d, j, tf.concat([i, j], 0), True))\n",
    "\n",
    "dataset_ano = dataset_ano1.concatenate(dataset_ano2).concatenate(dataset_ano3).concatenate(dataset_ano4)\n",
    "\n",
    "# All\n",
    "dataset_train_high = dataset_train_high.concatenate(dataset_ano).concatenate(dataset_train_mid)\n",
    "\n",
    "\n",
    "############# VAL\n",
    "# High Noise\n",
    "dataset_high = val_dataset_normal.map(file_high).map(lambda a, b, c, d, e, x, y: (a, x, y, False))\n",
    "dataset_high2 = val_dataset_normal.map(file_high).map(lambda a, b, c, d, e, x, y: (b, x, y, False))\n",
    "dataset_high3 = val_dataset_normal.map(file_high).map(lambda a, b, c, d, e, x, y: (c, x, y, False))\n",
    "dataset_high4 = val_dataset_normal.map(file_high).map(lambda a, b, c, d, e, x, y: (d, x, y, False))\n",
    "dataset_high5 = val_dataset_normal.map(file_high).map(lambda a, b, c, d, e, x, y: (e, x, y, False))\n",
    "\n",
    "dataset_val_high = dataset_high.concatenate(dataset_high2).concatenate(dataset_high3).concatenate(dataset_high4).concatenate(dataset_high5)\n",
    "\n",
    "# Low Noise\n",
    "dataset_low = val_dataset_anomaly.map(file_low).map(lambda a, b, c, d, e, x, y: (a, x, y, False))\n",
    "dataset_low2 = val_dataset_anomaly.map(file_low).map(lambda a, b, c, d, e, x, y: (b, x, y, False))\n",
    "dataset_low3 = val_dataset_anomaly.map(file_low).map(lambda a, b, c, d, e, x, y: (c, x, y, False))\n",
    "dataset_low4 = val_dataset_anomaly.map(file_low).map(lambda a, b, c, d, e, x, y: (d, x, y, False))\n",
    "dataset_low5 = val_dataset_anomaly.map(file_low).map(lambda a, b, c, d, e, x, y: (e, x, y, False))\n",
    "\n",
    "dataset_val_low = dataset_low.concatenate(dataset_low2).concatenate(dataset_low3).concatenate(dataset_low4).concatenate(dataset_low5)\n",
    "\n",
    "# Anomaly\n",
    "dataset_ano1 = val_dataset_real_anomaly.map(file_ano).map(lambda a, b, c, d, x, y, z, i, j: (a, j, tf.concat([x, j], 0), True))\n",
    "dataset_ano2 = val_dataset_real_anomaly.map(file_ano).map(lambda a, b, c, d, x, y, z, i, j: (b, j, tf.concat([y, j], 0), True))\n",
    "dataset_ano3 = val_dataset_real_anomaly.map(file_ano).map(lambda a, b, c, d, x, y, z, i, j: (c, j, tf.concat([z, j], 0), True))\n",
    "dataset_ano4 = val_dataset_real_anomaly.map(file_ano).map(lambda a, b, c, d, x, y, z, i, j: (d, j, tf.concat([i, j], 0), True))\n",
    "dataset_ano = dataset_ano1.concatenate(dataset_ano2).concatenate(dataset_ano3).concatenate(dataset_ano4)\n",
    "# All\n",
    "dataset_val_high = dataset_val_high.concatenate(dataset_val_low).concatenate(dataset_ano)\n",
    "\n",
    "\n",
    "# FINAL\n",
    "BATCH_SIZE=64\n",
    "train_dataset = dataset_train_high.map(to_mel).cache(\"tmp/cache\").shuffle(50000).batch(BATCH_SIZE).prefetch(1)\n",
    "val_dataset = dataset_val_high.map(to_mel).cache(\"tmp2/cache\").shuffle(12000).batch(BATCH_SIZE).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf8596cc-8f29-42a2-a86e-a1924a01fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinaryCrossEntropy_machine(y_true, y_pred):\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    term_0 = (1 - y_true) * K.log(1 - y_pred + K.epsilon())  \n",
    "    term_1 = y_true * K.log(y_pred + K.epsilon())\n",
    "    result = -K.mean(term_0 + term_1, axis=0)\n",
    "    return result\n",
    "\n",
    "def binary_crossentropy_test(y_true, y_pred):\n",
    "    batch_size = K.shape(y_true)[0]\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    term_0 = (1 - y_true[:,:-1]) * K.log(1 - y_pred + K.epsilon())  \n",
    "    term_1 = y_true[:,:-1] * K.log(y_pred + K.epsilon())\n",
    "    size = tf.cast(tf.size(term_0 + term_1), dtype=tf.float32) * tf.cast(tf.size(y_true[:,-1]), dtype=tf.float32)\n",
    "    machine_true = tf.reshape(y_true[:,-1], [batch_size, 1])\n",
    "    result = -K.mean(K.mean((term_0 + term_1)*machine_true, axis=0))\n",
    "    return result\n",
    "\n",
    "def BinaryCrossEntropy_section(y_true, y_pred): \n",
    "    batch_size = K.shape(y_true)[0]\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    term_0 = (1 - y_true[:,:-1]) * K.log(1 - y_pred + K.epsilon())  \n",
    "    term_1 = y_true[:,:-1] * K.log(y_pred + K.epsilon())\n",
    "    size = tf.cast(tf.size(term_0 + term_1), dtype=tf.float32) * tf.cast(tf.size(y_true[:,-1]), dtype=tf.float32)\n",
    "    machine_true = tf.reshape(y_true[:,-1], [batch_size, 1])\n",
    "    result = -tf.math.divide_no_nan(K.sum(K.sum((term_0 + term_1)*machine_true, axis=0)), (size + K.epsilon()))\n",
    "    return result\n",
    "\n",
    "def binary_accuracy_section(y_true, y_pred):\n",
    "    batch_size = K.shape(y_true)[0]\n",
    "    machine_true = tf.reshape(y_true[:,-1], [batch_size, 1])\n",
    "    return K.mean(K.equal(y_true[:,:-1]*machine_true, K.round(y_pred)), axis=-1)\n",
    "\n",
    "class DecayHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.lr = []\n",
    "        self.wd = []\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.lr.append(self.model.optimizer.lr(self.model.optimizer.iterations))\n",
    "        self.wd.append(self.model.optimizer.weight_decay)\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    Learning rate is scheduled to be reduced after 20, 30 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-4\n",
    "\n",
    "    if epoch >= 10:\n",
    "        lr *= 1e-1\n",
    "    elif epoch >= 20:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "def wd_schedule(epoch):\n",
    "    \"\"\"Weight Decay Schedule\n",
    "    Weight decay is scheduled to be reduced after 20, 30 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        wd (float32): weight decay\n",
    "    \"\"\"\n",
    "    wd = 1e-5\n",
    "\n",
    "    if epoch >= 20:\n",
    "        wd *= 1e-1\n",
    "    elif epoch >= 10:\n",
    "        wd *= 1e-1\n",
    "    print('Weight decay: ', wd)\n",
    "    return wd\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(os.path.join('logs', 'adamw'),\n",
    "                                             profile_batch=0)\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "\n",
    "def BinaryCrossEntropy_machine(y_true, y_pred):\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    term_0 = (1 - y_true) * K.log(1 - y_pred + K.epsilon())  \n",
    "    term_1 = y_true * K.log(y_pred + K.epsilon())\n",
    "    result = -K.mean(term_0 + term_1, axis=0)\n",
    "    return result\n",
    "\n",
    "def binary_crossentropy_test(y_true, y_pred):\n",
    "    batch_size = K.shape(y_true)[0]\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    term_0 = (1 - y_true[:,:-1]) * K.log(1 - y_pred + K.epsilon())  \n",
    "    term_1 = y_true[:,:-1] * K.log(y_pred + K.epsilon())\n",
    "    size = tf.cast(tf.size(term_0 + term_1), dtype=tf.float32) * tf.cast(tf.size(y_true[:,-1]), dtype=tf.float32)\n",
    "    machine_true = tf.reshape(y_true[:,-1], [batch_size, 1])\n",
    "    result = -K.mean(K.mean((term_0 + term_1)*machine_true, axis=0))\n",
    "    return result\n",
    "\n",
    "def categorical_cross_entropy(y_true, y_pred):\n",
    "    batch_size = K.shape(y_true)[0]\n",
    "    # y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    term_1 = y_true[:,:-1] * K.log(y_pred + K.epsilon())\n",
    "    machine_true = tf.reshape(y_true[:,-1], [batch_size, 1])\n",
    "    result = -K.mean(K.sum(term_1*machine_true, axis=0))\n",
    "    return result\n",
    "\n",
    "\n",
    "def BinaryCrossEntropy_section(y_true, y_pred): \n",
    "    batch_size = K.shape(y_true)[0]\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    term_0 = (1 - y_true[:,:-1]) * K.log(1 - y_pred + K.epsilon())  \n",
    "    term_1 = y_true[:,:-1] * K.log(y_pred + K.epsilon())\n",
    "    size = tf.cast(tf.size(term_0 + term_1), dtype=tf.float32) * tf.cast(tf.size(y_true[:,-1]), dtype=tf.float32)\n",
    "    machine_true = tf.reshape(y_true[:,-1], [batch_size, 1])\n",
    "    result = -tf.math.divide_no_nan(K.sum(K.sum((term_0 + term_1)*machine_true, axis=0)), (size + K.epsilon()))\n",
    "    return result\n",
    "\n",
    "def binary_accuracy_section(y_true, y_pred):\n",
    "    batch_size = K.shape(y_true)[0]\n",
    "    machine_true = tf.reshape(y_true[:,-1], [batch_size, 1])\n",
    "    return K.mean(K.equal(y_true[:,:-1]*machine_true, K.round(y_pred)), axis=-1)\n",
    "\n",
    "def categorical_accuracy_section(y_true, y_pred, class_to_ignore=0):\n",
    "    batch_size = K.shape(y_true)[0]\n",
    "    machine_true = tf.reshape(y_true[:,-1], [batch_size, 1])\n",
    "    return metrics_utils.sparse_categorical_matches(\n",
    "        tf.math.argmax(y_true*machine_true, axis=-1), y_pred\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f9e15f-6854-49c4-a264-359eb2409ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "============== MODEL TRAINING ==============\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 128, 128, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 64, 64, 64)   640         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64, 64, 64)  256         ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 64, 64, 64)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 64, 64, 64)   36928       ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 64, 64, 64)  256         ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 64, 64, 64)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 64, 64, 128)  8320        ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 64, 64, 128)  0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " depthwise_conv2d (DepthwiseCon  (None, 32, 32, 128)  1280       ['re_lu_2[0][0]']                \n",
      " v2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 128)  512        ['depthwise_conv2d[0][0]']       \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 32, 32, 128)  0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 64)   8256        ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 64)  256         ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 128)  8320        ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 128)  512        ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 32, 32, 128)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " depthwise_conv2d_1 (DepthwiseC  (None, 32, 32, 128)  1280       ['re_lu_4[0][0]']                \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 128)  512        ['depthwise_conv2d_1[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 32, 32, 128)  0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 64)   8256        ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 64)  256         ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 64)   0           ['batch_normalization_7[0][0]',  \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 128)  8320        ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 32, 32, 128)  512        ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 32, 32, 128)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " depthwise_conv2d_2 (DepthwiseC  (None, 32, 32, 128)  1280       ['re_lu_6[0][0]']                \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 32, 32, 128)  512        ['depthwise_conv2d_2[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 32, 32, 128)  0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 64)   8256        ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 32, 32, 64)  256         ['conv2d_7[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 64)   0           ['batch_normalization_10[0][0]', \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 32, 32, 128)  8320        ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 128)  512        ['conv2d_8[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)                 (None, 32, 32, 128)  0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_3 (DepthwiseC  (None, 32, 32, 128)  1280       ['re_lu_8[0][0]']                \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32, 32, 128)  512        ['depthwise_conv2d_3[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)                 (None, 32, 32, 128)  0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 32, 32, 64)   8256        ['re_lu_9[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 32, 32, 64)  256         ['conv2d_9[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 32, 32, 64)   0           ['batch_normalization_13[0][0]', \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 128)  8320        ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 32, 32, 128)  512        ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)                (None, 32, 32, 128)  0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_4 (DepthwiseC  (None, 32, 32, 128)  1280       ['re_lu_10[0][0]']               \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 32, 32, 128)  512        ['depthwise_conv2d_4[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)                (None, 32, 32, 128)  0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 64)   8256        ['re_lu_11[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 32, 32, 64)  256         ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 32, 32, 64)   0           ['batch_normalization_16[0][0]', \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 256)  16640       ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)                (None, 32, 32, 256)  0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_5 (DepthwiseC  (None, 16, 16, 256)  2560       ['re_lu_12[0][0]']               \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 16, 16, 256)  1024       ['depthwise_conv2d_5[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)                (None, 16, 16, 256)  0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 16, 16, 128)  32896       ['re_lu_13[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 16, 16, 128)  512        ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 16, 16, 256)  33024       ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 16, 16, 256)  1024       ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)                (None, 16, 16, 256)  0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_6 (DepthwiseC  (None, 8, 8, 256)   2560        ['re_lu_14[0][0]']               \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 8, 8, 256)   1024        ['depthwise_conv2d_6[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)                (None, 8, 8, 256)    0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 8, 8, 128)    32896       ['re_lu_15[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 8, 8, 128)   512         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 8, 8, 256)    33024       ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)                (None, 8, 8, 256)    0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_7 (DepthwiseC  (None, 8, 8, 256)   2560        ['re_lu_16[0][0]']               \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 8, 8, 256)   1024        ['depthwise_conv2d_7[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_17 (ReLU)                (None, 8, 8, 256)    0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 8, 8, 128)    32896       ['re_lu_17[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 8, 8, 128)   512         ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 8, 8, 128)    0           ['batch_normalization_25[0][0]', \n",
      "                                                                  'batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 8, 8, 256)    33024       ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_18 (ReLU)                (None, 8, 8, 256)    0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_8 (DepthwiseC  (None, 8, 8, 256)   2560        ['re_lu_18[0][0]']               \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 8, 8, 256)   1024        ['depthwise_conv2d_8[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_19 (ReLU)                (None, 8, 8, 256)    0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 8, 8, 128)    32896       ['re_lu_19[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 8, 8, 128)   512         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 8, 8, 128)    0           ['batch_normalization_28[0][0]', \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 8, 8, 256)    33024       ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_20 (ReLU)                (None, 8, 8, 256)    0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_9 (DepthwiseC  (None, 8, 8, 256)   2560        ['re_lu_20[0][0]']               \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 8, 8, 256)   1024        ['depthwise_conv2d_9[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_21 (ReLU)                (None, 8, 8, 256)    0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 8, 8, 128)    32896       ['re_lu_21[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 8, 8, 128)   512         ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 8, 8, 128)    0           ['batch_normalization_31[0][0]', \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 8, 8, 256)    33024       ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_22 (ReLU)                (None, 8, 8, 256)    0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_10 (Depthwise  (None, 8, 8, 256)   2560        ['re_lu_22[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 8, 8, 256)   1024        ['depthwise_conv2d_10[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_23 (ReLU)                (None, 8, 8, 256)    0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 8, 8, 128)    32896       ['re_lu_23[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 8, 8, 128)   512         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 8, 8, 128)    0           ['batch_normalization_34[0][0]', \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 8, 8, 256)    33024       ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_24 (ReLU)                (None, 8, 8, 256)    0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_11 (Depthwise  (None, 8, 8, 256)   2560        ['re_lu_24[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 8, 8, 256)   1024        ['depthwise_conv2d_11[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_25 (ReLU)                (None, 8, 8, 256)    0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 8, 8, 128)    32896       ['re_lu_25[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 8, 8, 128)   512         ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 8, 8, 128)    0           ['batch_normalization_37[0][0]', \n",
      "                                                                  'add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 8, 8, 512)    66048       ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 8, 8, 512)   2048        ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_26 (ReLU)                (None, 8, 8, 512)    0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_12 (Depthwise  (None, 4, 4, 512)   5120        ['re_lu_26[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 4, 4, 512)   2048        ['depthwise_conv2d_12[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_27 (ReLU)                (None, 4, 4, 512)    0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 4, 4, 128)    65664       ['re_lu_27[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 4, 4, 128)   512         ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 4, 4, 256)    33024       ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 4, 4, 256)   1024        ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_28 (ReLU)                (None, 4, 4, 256)    0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_13 (Depthwise  (None, 4, 4, 256)   2560        ['re_lu_28[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 4, 4, 256)   1024        ['depthwise_conv2d_13[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_29 (ReLU)                (None, 4, 4, 256)    0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 4, 4, 128)    32896       ['re_lu_29[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 4, 4, 128)   512         ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 4, 4, 256)    33024       ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 4, 4, 256)   1024        ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_30 (ReLU)                (None, 4, 4, 256)    0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_14 (Depthwise  (None, 4, 4, 256)   2560        ['re_lu_30[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 4, 4, 256)   1024        ['depthwise_conv2d_14[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_31 (ReLU)                (None, 4, 4, 256)    0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 4, 4, 128)    32896       ['re_lu_31[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 4, 4, 128)   512         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4, 4, 128)    0           ['batch_normalization_46[0][0]', \n",
      "                                                                  'batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 4, 4, 512)    66048       ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 4, 4, 512)   2048        ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_32 (ReLU)                (None, 4, 4, 512)    0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_15 (Depthwise  (None, 4, 4, 512)   8704        ['re_lu_32[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 4, 4, 512)   2048        ['depthwise_conv2d_15[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 4, 4, 128)    65664       ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " Dropout1 (Dropout)             (None, 4, 4, 128)    0           ['conv2d_33[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 128)         0           ['Dropout1[0][0]']               \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          16512       ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          16512       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " affine (Dense)                 (None, 1)            2           ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " linear (Dense)                 (None, 4)            512         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,076,226\n",
      "Trainable params: 1,056,898\n",
      "Non-trainable params: 19,328\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/300\n",
      "736/736 [==============================] - 99s 105ms/step - loss: 0.0033 - affine_loss: 0.0018 - linear_loss: 0.0015 - affine_binary_accuracy: 0.9999 - linear_binary_accuracy_section: 0.9741 - val_loss: 0.0062 - val_affine_loss: 0.0028 - val_linear_loss: 0.0034 - val_affine_binary_accuracy: 0.9997 - val_linear_binary_accuracy_section: 0.9769\n",
      "Epoch 2/300\n",
      "736/736 [==============================] - 77s 99ms/step - loss: 0.0030 - affine_loss: 0.0018 - linear_loss: 0.0012 - affine_binary_accuracy: 0.9998 - linear_binary_accuracy_section: 0.9758 - val_loss: 0.0060 - val_affine_loss: 0.0028 - val_linear_loss: 0.0032 - val_affine_binary_accuracy: 0.9995 - val_linear_binary_accuracy_section: 0.9748\n",
      "Epoch 3/300\n",
      "736/736 [==============================] - 75s 99ms/step - loss: 0.0030 - affine_loss: 0.0016 - linear_loss: 0.0014 - affine_binary_accuracy: 0.9999 - linear_binary_accuracy_section: 0.9715 - val_loss: 0.0054 - val_affine_loss: 0.0022 - val_linear_loss: 0.0031 - val_affine_binary_accuracy: 0.9997 - val_linear_binary_accuracy_section: 0.9745\n",
      "Epoch 4/300\n",
      "736/736 [==============================] - 79s 102ms/step - loss: 0.0027 - affine_loss: 0.0017 - linear_loss: 9.9498e-04 - affine_binary_accuracy: 0.9999 - linear_binary_accuracy_section: 0.9744 - val_loss: 0.0063 - val_affine_loss: 0.0028 - val_linear_loss: 0.0035 - val_affine_binary_accuracy: 0.9996 - val_linear_binary_accuracy_section: 0.9659\n",
      "Epoch 5/300\n",
      "736/736 [==============================] - 78s 102ms/step - loss: 0.0027 - affine_loss: 0.0017 - linear_loss: 0.0010 - affine_binary_accuracy: 0.9999 - linear_binary_accuracy_section: 0.9790 - val_loss: 0.0061 - val_affine_loss: 0.0028 - val_linear_loss: 0.0033 - val_affine_binary_accuracy: 0.9996 - val_linear_binary_accuracy_section: 0.9806\n",
      "Epoch 6/300\n",
      "736/736 [==============================] - 77s 102ms/step - loss: 0.0024 - affine_loss: 0.0015 - linear_loss: 8.8767e-04 - affine_binary_accuracy: 0.9999 - linear_binary_accuracy_section: 0.9818 - val_loss: 0.0096 - val_affine_loss: 0.0025 - val_linear_loss: 0.0071 - val_affine_binary_accuracy: 0.9996 - val_linear_binary_accuracy_section: 0.9727\n",
      "Epoch 7/300\n",
      "736/736 [==============================] - 78s 102ms/step - loss: 0.0032 - affine_loss: 0.0020 - linear_loss: 0.0012 - affine_binary_accuracy: 0.9998 - linear_binary_accuracy_section: 0.9853 - val_loss: 0.0056 - val_affine_loss: 0.0020 - val_linear_loss: 0.0036 - val_affine_binary_accuracy: 0.9996 - val_linear_binary_accuracy_section: 0.9911\n",
      "Epoch 8/300\n",
      "736/736 [==============================] - 75s 100ms/step - loss: 0.0021 - affine_loss: 0.0014 - linear_loss: 7.0855e-04 - affine_binary_accuracy: 1.0000 - linear_binary_accuracy_section: 0.9889 - val_loss: 0.0050 - val_affine_loss: 0.0021 - val_linear_loss: 0.0030 - val_affine_binary_accuracy: 0.9997 - val_linear_binary_accuracy_section: 0.9919\n",
      "Epoch 9/300\n",
      "736/736 [==============================] - 76s 98ms/step - loss: 0.0022 - affine_loss: 0.0015 - linear_loss: 6.8925e-04 - affine_binary_accuracy: 0.9999 - linear_binary_accuracy_section: 0.9905 - val_loss: 0.0057 - val_affine_loss: 0.0022 - val_linear_loss: 0.0034 - val_affine_binary_accuracy: 0.9996 - val_linear_binary_accuracy_section: 0.9897\n",
      "Epoch 10/300\n",
      "736/736 [==============================] - 79s 104ms/step - loss: 0.0021 - affine_loss: 0.0016 - linear_loss: 5.0176e-04 - affine_binary_accuracy: 0.9999 - linear_binary_accuracy_section: 0.9848 - val_loss: 0.0055 - val_affine_loss: 0.0021 - val_linear_loss: 0.0034 - val_affine_binary_accuracy: 0.9997 - val_linear_binary_accuracy_section: 0.9866\n",
      "Epoch 11/300\n",
      "154/736 [=====>........................] - ETA: 54s - loss: 0.0015 - affine_loss: 0.0013 - linear_loss: 2.6481e-04 - affine_binary_accuracy: 1.0000 - linear_binary_accuracy_section: 0.9857"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Load Checkpoint\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(model\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory)\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmachine_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_2.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     42\u001b[0m model\u001b[38;5;241m.\u001b[39msave(model_file_path)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# number of vectors for each wave file\n",
    "# train model\n",
    "\n",
    "\n",
    "initial_learning_rate = 0.0005\n",
    "final_learning_rate = 0.00001\n",
    "learning_rate_decay_factor = (final_learning_rate / initial_learning_rate)**(1/200)\n",
    "steps_per_epoch = int(736)\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                initial_learning_rate=initial_learning_rate,\n",
    "                decay_steps=steps_per_epoch,\n",
    "                decay_rate=learning_rate_decay_factor,\n",
    "                staircase=True)\n",
    "\n",
    "\n",
    "print(\"============== MODEL TRAINING ==============\")\n",
    "model = keras_model_facenet.get_model((128,128,1), num_class=4)\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.00005), loss = \n",
    "              {\"affine\":\"binary_crossentropy\", \"linear\":binary_crossentropy_test},\n",
    "              metrics= {\"affine\"  :'binary_accuracy', \"linear\": binary_accuracy_section},\n",
    "              loss_weights=[1, 1])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Load Checkpoint\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=300,\n",
    "                    verbose=1,\n",
    "                    validation_data = val_dataset,\n",
    "                    callbacks = [checkpoint])\n",
    "\n",
    "\n",
    "pd.DataFrame.from_dict(model.history.history).to_csv(f'history_{machine_type}_2.csv',index=False)\n",
    "\n",
    "\n",
    "model.save(model_file_path)\n",
    "com.logger.info(\"save_model -> {}\".format(model_file_path))\n",
    "\n",
    "print(\"============== END TRAINING ==============\")\n",
    "\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e80a6-2b4a-4397-9a34-61ce866cfa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed0f59b-b185-40df-a2b1-96d1b3cd5ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208118b9-9bf2-4044-a146-692bd0cb33d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"saved_models_facenet/model_35+18.h5\", custom_objects={'BinaryCrossEntropy_machine': BinaryCrossEntropy_machine, 'BinaryCrossEntropy_section': BinaryCrossEntropy_section })\n",
    "model_output = model.get_layer('re_lu').output\n",
    "m = keras.Model(inputs=model.input, outputs=model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc7efc-f76f-4e54-95c9-75c8ea90ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ano(file_path):\n",
    "  # Convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    parts = tf.strings.split(parts[-1], sep='_')\n",
    "    return parts[4]\n",
    "\n",
    "def file_to_vectors(file_name):\n",
    "    \"\"\"\n",
    "    convert file_name to a vector array.\n",
    "\n",
    "    file_name : str\n",
    "        target .wav file\n",
    "\n",
    "    return : numpy.array( numpy.array( float ) )\n",
    "        vector array\n",
    "        * dataset.shape = (dataset_size, feature_vector_length)\n",
    "    \"\"\"\n",
    "    table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "    keys=tf.constant([2, 3, 4, 5, 1, 6, 7, 8]),\n",
    "    values=tf.constant([0, 1, 2, 3, 0 , 1, 2, 3]),\n",
    "    ),\n",
    "    default_value=tf.constant(-1),\n",
    "    name=\"class_weight\"\n",
    "    )\n",
    "    name = get_name(file_name)\n",
    "    # generate melspectrogram using tf.io\n",
    "    file_contents = tf.io.read_file(file_name)\n",
    "    data, sr = tf.audio.decode_wav(file_contents, desired_channels=1)\n",
    "    data = tf.squeeze(data, axis=[-1])\n",
    "    n = tf.split(data, 5)\n",
    "    return n[0],n[1],n[2],n[3],n[4], name\n",
    "\n",
    "def file_to_vectors2(file_name):\n",
    "    \"\"\"\n",
    "    convert file_name to a vector array.\n",
    "\n",
    "    file_name : str\n",
    "        target .wav file\n",
    "\n",
    "    return : numpy.array( numpy.array( float ) )\n",
    "        vector array\n",
    "        * dataset.shape = (dataset_size, feature_vector_length)\n",
    "    \"\"\"\n",
    "    table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "    keys=tf.constant([2, 3, 4, 5, 1, 6, 7, 8]),\n",
    "    values=tf.constant([0, 1, 2, 3, 0 , 1, 2, 3]),\n",
    "    ),\n",
    "    default_value=tf.constant(-1),\n",
    "    name=\"class_weight\"\n",
    "    )\n",
    "    name = get_name(file_name)\n",
    "    # generate melspectrogram using tf.io\n",
    "    file_contents = tf.io.read_file(file_name)\n",
    "    data, sr = tf.audio.decode_wav(file_contents, desired_channels=1)\n",
    "    data = tf.squeeze(data, axis=[-1])\n",
    "    n = tf.split(data, 5)\n",
    "    return n[0],n[1],n[2],n[3],n[4], name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def to_mel2(y, name,\n",
    "        n_mels=128,\n",
    "        n_fft=2823,\n",
    "        hop_length=347,\n",
    "        power=2.0,\n",
    "        sr=22050):\n",
    "    spectrogram = tfio.audio.spectrogram(y, nfft=n_fft, window=n_fft, stride=hop_length)\n",
    "    mel_spectrogram = tfio.audio.melscale(spectrogram, sr, mels=n_mels, fmin=0, fmax=11025, name=None)\n",
    "    dbscale_mel_spectrogram = tfio.audio.dbscale(mel_spectrogram, top_db=120)\n",
    "    dbscale_mel_spectrogram = tf.expand_dims(dbscale_mel_spectrogram, axis=2)\n",
    "    return dbscale_mel_spectrogram, name\n",
    "\n",
    "\n",
    "pos5 = tf.data.Dataset.list_files(f\"dev_data/low_noise/test/*.wav\")\n",
    "dataset_target = pos5.map(file_to_vectors2).map(lambda a, b, c, d, e, x: (a, x))\n",
    "dataset_target2 = pos5.map(file_to_vectors2).map(lambda a, b, c, d, e, x: (b, x))\n",
    "dataset_target3 = pos5.map(file_to_vectors2).map(lambda a, b, c, d, e, x: (c, x))\n",
    "dataset_target4 = pos5.map(file_to_vectors2).map(lambda a, b, c, d, e, x: (d, x))\n",
    "dataset_target5 = pos5.map(file_to_vectors2).map(lambda a, b, c, d, e, x: (e, x))\n",
    "\n",
    "dataset_train_low_gmm = dataset_target.concatenate(dataset_target2).concatenate(dataset_target3).concatenate(dataset_target4).concatenate(dataset_target5).map(to_mel2)\n",
    "# train_gmm_label = dataset_train_low_gmm.map(lambda a, x: x)\n",
    "# dataset_train_low_gmm = dataset_train_low_gmm.map(lambda a, x: a)\n",
    "\n",
    "# HIGH\n",
    "pos6 = tf.data.Dataset.list_files(f\"dev_data/high_noise/test/*.wav\")\n",
    "dataset_target = pos6.map(file_to_vectors).map(lambda a, b, c, d, e, x: (a, x))\n",
    "dataset_target2 = pos6.map(file_to_vectors).map(lambda a, b, c, d, e, x: (b, x))\n",
    "dataset_target3 = pos6.map(file_to_vectors).map(lambda a, b, c, d, e, x: (c, x))\n",
    "dataset_target4 = pos6.map(file_to_vectors).map(lambda a, b, c, d, e, x: (d, x))\n",
    "dataset_target5 = pos6.map(file_to_vectors).map(lambda a, b, c, d, e, x: (e, x))\n",
    "\n",
    "dataset_train_high_gmm = dataset_target.concatenate(dataset_target2).concatenate(dataset_target3).concatenate(dataset_target4).concatenate(dataset_target5).map(to_mel2)\n",
    "# train_gmm_label2 = dataset_train_high_gmm.map(lambda a, x: x)\n",
    "# train_gmm_label = train_gmm_label.concatenate(train_gmm_label2)\n",
    "# dataset_train_high_gmm = dataset_train_high_gmm.map(lambda a,x: a)\n",
    "\n",
    "\n",
    "train_gmm = dataset_train_high_gmm.concatenate(dataset_train_low_gmm).batch(1).prefetch(1)\n",
    "train_label = train_gmm.map(lambda a, x: x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f7fb70-fc3d-4a08-9de1-62756b277115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, random_state=123,learning_rate='auto')\n",
    "z = tsne.fit_transform(features)\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# train2 = tfds.as_dataframe(train.take(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b3086c-50f6-4f44-a63e-b7428274d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gmm_label = train_gmm_label.batch(1).prefetch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779847c2-8fe8-4565-975c-7d0800c8c2e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_gmm)\n",
    "for x in train_gmm.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f11e4-d9c8-4bee-9ce2-692fda717d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"y\"] = train_gmm_label\n",
    "df[\"comp-1\"] = z[:,0]\n",
    "df[\"comp-2\"] = z[:,1]\n",
    "df.to_csv('ibuki_new_correct.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4376ee82-39fe-4e64-9806-f227900600c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(features.shape)\n",
    "df2 = pd.read_csv('ibuki_new_correct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b9fdbb-a806-41cb-9578-12d635ffcde2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sns.set(rc={'figure.figsize':(12,12)})\n",
    "sns.scatterplot(x=\"comp-1\", y=\"comp-2\",\n",
    "                data=df).set(title=\"MNIST data T-SNE projection\")\n",
    "                # palette=sns.color_palette(\"hls\", 4), hue=df2.y.tolist(),\n",
    "\n",
    "# plt.ylim(-2, 2)\n",
    "# plt.xlim(-2, 2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea3eb6-ce53-4a07-8658-44c96c76103d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "model = keras.models.load_model(\"saved_models_facenet/model_53+008.h5\", custom_objects={'BinaryCrossEntropy_machine': BinaryCrossEntropy_machine, 'binary_accuracy_section':binary_accuracy_section, 'BinaryCrossEntropy_section': BinaryCrossEntropy_section })\n",
    "model_output = model.get_layer('re_lu_15').output\n",
    "\n",
    "m = keras.Model(inputs=model.input, outputs=model_output)\n",
    "m.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc28474-1555-4431-abe5-e8783a42ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = m.predict(train_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e4876a-234a-4e89-a9d9-7cd265b8ee71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb3afa0-fbe6-43d5-8f9d-838de4171fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a096b86-6626-43d5-8bea-d17f489d4a70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################## FOR AUC ROC BASED ON ACCURACY #####################################################\n",
    "def file_ano2(file_name):\n",
    "    \"\"\"\n",
    "    convert file_name to a vector array.\n",
    "\n",
    "    file_name : str\n",
    "        target .wav file\n",
    "\n",
    "    return : numpy.array( numpy.array( float ) )\n",
    "        vector array\n",
    "        * dataset.shape = (dataset_size, feature_vector_length)\n",
    "    \"\"\"\n",
    "    table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "    keys=tf.constant([2, 3, 4, 5, 1, 6, 7, 8]),\n",
    "    values=tf.constant([0, 1, 2, 3, 0 , 1, 2, 3]),\n",
    "    ),\n",
    "    default_value=tf.constant(-1),\n",
    "    name=\"class_weight\"\n",
    "    )\n",
    "    name = get_name(file_name)\n",
    "    # generate melspectrogram using tf.io\n",
    "    file_contents = tf.io.read_file(file_name)\n",
    "    data, sr = tf.audio.decode_wav(file_contents, desired_channels=1)\n",
    "    data = tf.squeeze(data, axis=[-1])\n",
    "    # label1 = tf.one_hot(int(label1), 8)\n",
    "    return data, name\n",
    "\n",
    "dataset_high = val_dataset1.map(file_to_vectors).map(lambda a, b, c, d, e, x: (a, x))\n",
    "dataset_high2 = val_dataset1.map(file_to_vectors).map(lambda a, b, c, d, e, x: (b, x))\n",
    "dataset_high3 = val_dataset1.map(file_to_vectors).map(lambda a, b, c, d, e, x: (c, x))\n",
    "dataset_high4 = val_dataset1.map(file_to_vectors).map(lambda a, b, c, d, e, x: (d, x))\n",
    "dataset_high5 = val_dataset1.map(file_to_vectors).map(lambda a, b, c, d, e, x: (e, x))\n",
    "\n",
    "dataset_val_high = dataset_high.concatenate(dataset_high2).concatenate(dataset_high3).concatenate(dataset_high4).concatenate(dataset_high5).map(to_mel2)\n",
    "\n",
    "# Anomaly\n",
    "dataset_ano = val_dataset3.map(file_ano2).map(to_mel2)\n",
    "dataset_val_high = dataset_val_high.concatenate(dataset_ano)\n",
    "\n",
    "# Low Noise\n",
    "dataset_low = val_dataset2.map(file_to_vectors2).map(lambda a, b, c, d, e, x: (a, x))\n",
    "dataset_low2 = val_dataset2.map(file_to_vectors2).map(lambda a, b, c, d, e, x: (b, x))\n",
    "dataset_low3 = val_dataset2.map(file_to_vectors2).map(lambda a, b, c, d, e, x: (c, x))\n",
    "dataset_low4 = val_dataset2.map(file_to_vectors2).map(lambda a, b, c, d, e, x: (d, x))\n",
    "dataset_low5 = val_dataset2.map(file_to_vectors2).map(lambda a, b, c, d, e, x: (e, x))\n",
    "\n",
    "dataset_val_low = dataset_low.concatenate(dataset_low2).concatenate(dataset_low3).concatenate(dataset_low4).concatenate(dataset_low5).map(to_mel2)\n",
    "val_dataset_test = dataset_val_low.concatenate(dataset_val_high).batch(1).prefetch(1)\n",
    "\n",
    "machine, section = model.predict(val_dataset_test)\n",
    "df = pd.DataFrame()\n",
    "df[\"name\"] = val_dataset_test.map(lambda x, y: y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bd169f-33ec-42ce-8319-45e25348f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Machine_pred\"] = [x for x in machine]\n",
    "df[\"Section_pred\"] = [x for x in section]\n",
    "df.to_csv('AUC_ROC.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8deaa83-5756-4828-97dc-552fe1a6842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## TSNE #####################################################\n",
    "tsne = TSNE(n_components=2, verbose=1, random_state=123, init='pca', learning_rate='auto')\n",
    "z = tsne.fit_transform(feature)\n",
    "df = pd.DataFrame()\n",
    "df[\"y\"] = train_label\n",
    "df[\"comp-1\"] = z[:,0]\n",
    "df[\"comp-2\"] = z[:,1]\n",
    "df.to_csv('test_only_random_10.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf77f74-7c18-4eab-ae29-0d13dacdc0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df2 = pd.read_csv('test_only_random_10.csv')\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "sns.scatterplot(x=\"comp-1\", y=\"comp-2\",\n",
    "                palette=sns.color_palette(\"hls\", 4), hue=df2.y.tolist(),\n",
    "                data=df).set(title=\"MNIST data T-SNE projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d770561f-df81-4538-8ae6-4c2c210d4752",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install visualkeras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbfac5e-9067-430d-a396-cd72568448e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "from PIL import ImageFont\n",
    "import keras\n",
    "model2 = keras.models.load_model(\"model_car1.hdf5\")\n",
    "visualkeras.layered_view(model2, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f4283d-1355-4bdb-9428-f4fcf624d839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51598b3b-d918-4f2b-958d-13290ad22b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# VAL\n",
    "dataset_high = val_dataset1.map(file_high).map(lambda a, b, c, d, e, x, y: (a, 1.0))\n",
    "dataset_high2 = val_dataset1.map(file_high).map(lambda a, b, c, d, e, x, y: (b, x, y))\n",
    "dataset_high3 = val_dataset1.map(file_high).map(lambda a, b, c, d, e, x, y: (c, x, y))\n",
    "dataset_high4 = val_dataset1.map(file_high).map(lambda a, b, c, d, e, x, y: (d, x, y))\n",
    "dataset_high5 = val_dataset1.map(file_high).map(lambda a, b, c, d, e, x, y: (e, x, y))\n",
    "\n",
    "dataset_val_high = dataset_high.concatenate(dataset_high2).concatenate(dataset_high3).concatenate(dataset_high4).concatenate(dataset_high5).map(to_mel)\n",
    "\n",
    "# Anomaly\n",
    "dataset_ano = val_dataset3.map(file_ano).map(to_mel)\n",
    "dataset_val_high = dataset_val_high.concatenate(dataset_ano)\n",
    "\n",
    "# Low Noise\n",
    "dataset_low = val_dataset2.map(file_low).map(lambda a, b, c, d, e, x, y: (a, x, y))\n",
    "dataset_low2 = val_dataset2.map(file_low).map(lambda a, b, c, d, e, x, y: (b, x, y))\n",
    "dataset_low3 = val_dataset2.map(file_low).map(lambda a, b, c, d, e, x, y: (c, x, y))\n",
    "dataset_low4 = val_dataset2.map(file_low).map(lambda a, b, c, d, e, x, y: (d, x, y))\n",
    "dataset_low5 = val_dataset2.map(file_low).map(lambda a, b, c, d, e, x, y: (e, x, y))\n",
    "\n",
    "dataset_val_low = dataset_low.concatenate(dataset_low2).concatenate(dataset_low3).concatenate(dataset_low4).concatenate(dataset_low5).map(to_mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25bbd39-5836-40aa-96d5-8cff9d217477",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
